{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and minibatch MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet   \n",
    "using Compat,GZip\n",
    "using Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Where to download mnist from\"\n",
    "mnisturl = \"http://yann.lecun.com/exdb/mnist\"\n",
    "\n",
    "\"Where to download mnist to\"\n",
    "mnistdir = \"MNIST/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gzload(file; path=\"$file\", url=\"http://yann.lecun.com/exdb/mnist/$file\")\n",
    "    isfile(path) || download(url, path)\n",
    "    f = gzopen(path)\n",
    "    a = @compat read(f)\n",
    "    close(f)\n",
    "    return(a)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That function is going to load raw data to the ram so that you will be able to use it\n",
    "function loaddata()\n",
    "    info(\"Loading MNIST...\")\n",
    "    # path = joinpath(mnistdir,file) etc.\n",
    "    xtrn = gzload(\"MNIST/train-images-idx3-ubyte.gz\")[17:end]\n",
    "    xtst = gzload(\"MNIST/t10k-images-idx3-ubyte.gz\")[17:end]\n",
    "    ytrn = gzload(\"MNIST/train-labels-idx1-ubyte.gz\")[9:end]\n",
    "    ytst = gzload(\"MNIST/t10k-labels-idx1-ubyte.gz\")[9:end]\n",
    "    return (xtrn, ytrn, xtst, ytst)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrnraw, ytrn, xtstraw, ytst = loaddata(); # These are raw data and not suitable to direct usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistview(x,i)=colorview(Gray,permutedims(x[:,:,1,i],(2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrn = convert(Array{Float32}, reshape(xtrnraw ./ 255, (28,28,1, div(length(xtrnraw), 784))));\n",
    "xtst = convert(Array{Float32}, reshape(xtstraw ./ 255, (28,28,1, div(length(xtstraw), 784))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent 0 as 10th class\n",
    "ytrn[ytrn.==0]=10;\n",
    "ytst[ytst.==0]=10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcat([mnistview(xtst,i) for i=1:5]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrn[1:5], ytst[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atype = KnetArray{Float32}\n",
    "Ctype = Array{Float32}\n",
    "gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nb = 80  # batch size\n",
    "dtst = minibatch(xtst,ytst,Nb;xtype=Atype) # [ (x1,y1), (x2,y2), ... ] where xi,yi are minibatches of 100\n",
    "dtrn = minibatch(xtrn,ytrn,Nb;xtype=Atype) # [ (x1,y1), (x2,y2), ... ] where xi,yi are minibatches of 100\n",
    "length(dtrn),length(dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first minibatch\n",
    "map(summary,first(dtst))  # (x,y) pair where x: 4-D Float32 array with X,Y,C,N  y: 1-D integer array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y) = first(dtst);\n",
    "y[1:5]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MNIST using MLP with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "zeroone(w,data,predict) = 1 - accuracy(w,data,predict)\n",
    "loss(w,data,predict) = mean(loss(w,x,y,predict) for (x,y) in data)\n",
    "loss(w,x,y,predict; o...) = nll(predict(w,x;o...),y)\n",
    "lossgrad = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mlpdrop(w,x; pdrop=(0,0))\n",
    "    x = mat(x)\n",
    "    x = dropout(x,pdrop[1])\n",
    "    for i=1:2:length(w)-2\n",
    "        x = relu.(w[i]*x .+ w[i+1])\n",
    "        x = dropout(x,pdrop[2])\n",
    "    end\n",
    "    return w[end-1]*x .+ w[end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function winit(h...; std=0.01, x=28*28, y=10, atype=gpu()>=0 ? KnetArray{Float32} : Array{Float32})\n",
    "    h = [x, h..., y]   # use winit(h1,h2,...,hn) for n hidden layer mlp\n",
    "    w = Any[]\n",
    "    for i=1:length(h)-1\n",
    "        push!(w, std*randn(h[i+1],h[i]))\n",
    "        push!(w, zeros(h[i+1],1))\n",
    "    end\n",
    "    map(atype, w)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = winit(64)  # weights and biases for an MLP with a single hidden layer of size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(wts,x,y,mlpdrop)  # Average loss for a single (x,y) minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(wts,dtst,mlpdrop)  # Average loss for the whole test set for current wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model(w) with SGD and return a list containing w for every epoch\n",
    "function train(w,data,predict; epochs=3,lr=0.15,o...)\n",
    "    weights = Any[deepcopy(w)]\n",
    "    #opts = map(x->Sgd(lr=lr), w)  # SGD with default learning rate\n",
    "    opts = map(x->Adam(), w)\n",
    "    for epoch in 1:epochs\n",
    "        for (x,y) in data\n",
    "            g = lossgrad(w,x,y,predict;o...)\n",
    "            update!(w,g,opts)\n",
    "        end\n",
    "        push!(weights,deepcopy(w))\n",
    "    end\n",
    "    return weights\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(1)\n",
    "@time trn1=train(winit(48,16,x=28*28),dtrn,mlpdrop;epochs=10,lr=0.15,pdrop=(0.2,0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time trnloss1 = [ loss(w,dtrn,mlpdrop) for w in trn1 ]\n",
    "@time tstloss1 = [ loss(w,dtst,mlpdrop) for w in trn1 ]\n",
    "@time trnerr1 = [ zeroone(w,dtrn,mlpdrop) for w in trn1 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time tsterr1 = [ zeroone(w,dtst,mlpdrop) for w in trn1 ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(trnerr1),minimum(tsterr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = trn1 = trnloss1 = tstloss1 = trnerr1 = tsterr1 = nothing; knetgc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MNIST using a Baseline CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function convnet(w,x; activation=(relu,relu), pdrop=(0,0,0))\n",
    "    for i=1:2:length(w)  # pdrop[1]:input, pdrop[2]:conv, pdrop[3]:fcc layer\n",
    "        if ndims(w[i]) == 4     # 4 params means convolutional layer\n",
    "            x = dropout(x, pdrop[i==1?1:2])\n",
    "            x = conv4(w[i], x, padding=1) .+ w[i+1]\n",
    "            x = pool(activation[1].(x))\n",
    "        elseif ndims(w[i]) == 2 # fully connected layer\n",
    "            if i == length(w)-1; x = dropout(x, pdrop[i==1?1:3]); end\n",
    "            # x = dropout(x, pdrop[i==1?1:3])  Hinton used dropout only in the final FC layer!\n",
    "            x = w[i]*mat(x) .+ w[i+1]\n",
    "            if i < length(w)-1; x = activation[2].(x); end\n",
    "        else\n",
    "            error(\"Unknown layer type: $(size(w[i]))\")\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization for multiple layers\n",
    "# h[i] is an integer for a fully connected layer, a triple of integers for convolution filters\n",
    "# Output is an array [w0,b0,w1,b1,...,wn,bn] where wi,bi is the weight matrix/tensor and bias vector for the i'th layer\n",
    "function cinit(h...)  # use cinit(x,h1,h2,...,hn,y) for n hidden layer model\n",
    "    w = Any[]\n",
    "    x = h[1]\n",
    "    for i=2:length(h)\n",
    "        if isa(h[i],Tuple)\n",
    "            (x1,x2,cx) = x\n",
    "            (w1,w2,cy) = h[i]\n",
    "            push!(w, xavier(w1,w2,cx,cy))\n",
    "            push!(w, zeros(1,1,cy,1))\n",
    "            x = (div(x1-w1+1+2,2),div(x2-w2+1+2,2),cy)\n",
    "        elseif isa(h[i],Integer)\n",
    "            push!(w, xavier(h[i],prod(x)))\n",
    "            push!(w, zeros(h[i],1))\n",
    "            x = h[i]\n",
    "        else\n",
    "            error(\"Unknown layer type: $(h[i])\")\n",
    "        end\n",
    "    end\n",
    "    map(Atype, w)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnbase=cinit((28,28,1), (5,5,256), (5,5,256), (5,5,128), 328, 192, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y) = first(dtst)\n",
    "loss(cnnbase,x,y,convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnbase = weights = trnloss = tstloss = trnerr = tsterr = nothing; knetgc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(1)\n",
    "cnnbase=cinit((28,28,1), (5,5,256), (5,5,256), (5,5,128), 328, 192, 10)\n",
    "@time weights = train(cnnbase,dtrn,convnet;epochs=5,lr=0.15,pdrop=(0,0,0.30))\n",
    "@time trnloss = [ loss(w,dtrn,convnet) for w in weights ]\n",
    "@time tstloss = [ loss(w,dtst,convnet) for w in weights ]\n",
    "@time trnerr = [ zeroone(w,dtrn,convnet) for w in weights ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time tsterr = [ zeroone(w,dtst,convnet) for w in weights ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(tstloss),minimum(tsterr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plotly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr tsterr],ylim=(0,1.0), linewidth=3,\n",
    "    labels=[:trainErr :testErr],xlabel=\"Epochs\", ylabel=\"CNN Error with 30% dropout\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnerr, tsterr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnbase = weights = trnloss = tstloss = trnerr = tsterr = nothing; knetgc()\n",
    "# Knet.gpuinfo() # Knet.meminfo() # Knet.memdbg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Capsule Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Atype, Ctype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nclass = 10  # number of classes\n",
    "Vprimes = 32  # number of primary capsules vertically stacked (along z-axis), focusing on the same segment of images\n",
    "Nax1primes = 6  # number of primary capsules along axis-1 (y-axis) of an image: Julia is column-major!\n",
    "Nax2primes = 6  # number of primary capsules along axis-2 (x-axis) of an image\n",
    "Nsegm = Nax1primes*Nax2primes  # number of segments per image: one primary capsule for each segment of each image\n",
    "Nprimes = Nsegm*Vprimes  # total number of primary capsule\n",
    "Dprime = 8  # dimension of a primary capsules\n",
    "Dsecond = 16 # dimension of a secondary (higher layer) capsule\n",
    "Nchannels = Vprimes*Dprime  # number of conv.layer channels to fit Vprimes capsules of dim. Dprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function winitdecoder(h...; x=Nclass*Dsecond, y=28*28) \n",
    "    h = [x, h..., y]   # use winit(h1,h2,...,hn) for n-hidden layer MLP\n",
    "    w = Any[]\n",
    "    for i=1:length(h)-1\n",
    "        push!(w, xavier(h[i+1],h[i]))\n",
    "        push!(w, zeros(h[i+1],1))\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function wtsinit()\n",
    "    wts = Any[ xavier(9,9,1,Nchannels),  zeros(1,1,Nchannels,1), \n",
    "        xavier(9,9,Nchannels,Nchannels), zeros(1,1,Nchannels,1)]\n",
    "    W = 0.1*randn(Dprime,Nprimes,Nclass,Dsecond) \n",
    "    push!(wts,W)\n",
    "    append!(wts, winitdecoder(512,1024))  \n",
    "    wts = map(Atype, wts)\n",
    "\n",
    "    return wts\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function convLayer(w, x, strides=(1,2))\n",
    "    # dropouts =(0,0)\n",
    "    # paddings = (0,0)\n",
    "    # strides = (1,2)\n",
    "    for i=1:2:length(w)\n",
    "        # x = dropout(x, dropouts[i==1?1:2])\n",
    "        x = conv4(w[i], x, stride=strides[i==1?1:2]) .+ w[i+1]\n",
    "        x = relu.(x)\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softMax(X; axis=2) \n",
    "    X = X .- maximum(X, axis)\n",
    "    prob = exp.(X) ./ sum(exp.(X), axis)\n",
    "    return prob\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function safeNorm(s; axis=4, eps=1e-7)\n",
    "    sNorm2 = sum(abs2.(s),axis)\n",
    "    sNorm = sqrt.(sNorm2+eps)\n",
    "    return sNorm\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function squash(s; axis=4, eps=1e-7)    \n",
    "    sNorm2 = sum(abs2.(s),axis)\n",
    "    sNorm = sqrt.(sNorm2+eps)\n",
    "    #sNorm, sNorm2 = safeNorm(s,axis=axis)\n",
    "    sUnit = s ./ sNorm\n",
    "    sFactor = sNorm2./(sNorm2.+1)\n",
    "    V = sFactor.*sUnit\n",
    "    return V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function maxidx(matrix)  # find index of maximums along axis-1 (columns)\n",
    "    M = size(matrix,1)\n",
    "    N = size(matrix,2)  \n",
    "    idxes = zeros(Int8,N)\n",
    "    maxes = maximum(matrix, 1)\n",
    "\n",
    "    for j=1:N\n",
    "        for i=1:M\n",
    "            if maxes[j]==matrix[i,j]\n",
    "            idxes[j] = i\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return idxes\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y) = first(dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(summary,first(dtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtscap = nothing; knetgc();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1hot(y) = convert(KnetArray{Float32}, sparse(convert(Vector{Int},y),1:length(y),one(eltype(y)),Nclass,length(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function decode(w,vtodeco)  # decoder for reconstruction\n",
    "    for i=1:2:length(w)-2\n",
    "        vtodeco = relu.(w[i]*vtodeco .+ w[i+1])\n",
    "    end\n",
    "    return sigm.(w[end-1]*vtodeco .+ w[end])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mask(v, y1ht)  # filter for reconstruction\n",
    "    vmasked = permutedims(y1ht.*v , (1,3,2))\n",
    "    vtodecode = reshape(vmasked,(Dsecond*Nclass,Nb))\n",
    "    return vtodecode\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function capsnet(w, x)\n",
    "    con = convLayer(w[1:4],x)\n",
    "    pri = reshape(con,(Nax1primes,Nax2primes,Dprime,Vprimes,Nb))  # pri = reshape(con,(6,6,8,32,80))\n",
    "    pri = permutedims(pri,(1,2,4,3,5))\n",
    "    \n",
    "    pri = reshape(pri,(Nprimes,Dprime,Nb))  # pri = reshape(pri,(1152,8,80))\n",
    "    pri = permutedims(pri,(2,1,3))\n",
    "    pri = squash(pri, axis=1)  # along the contents of primary capsules\n",
    "    \n",
    "    pri = reshape(pri,(Dprime,Nprimes,1,1,Nb))  # pri = reshape(pri,(8,1152,1,1,80))\n",
    "\n",
    "    UHat = convert(Atype, ones(Dprime,Nprimes,Nclass,Dsecond,Nb))  # UHat = convert(Atype, ones(8,1152,10,16,80))\n",
    "    W = w[5].*UHat  # achieved tiling to higher dimensions by multiplying ones!\n",
    "    \n",
    "    UHat = pri.*W  # prediction vectors\n",
    "    UHat = sum(UHat,1)  # achieved affine transformations without matmul()\n",
    "    UHat = permutedims(reshape(UHat,(Nprimes,Nclass,Dsecond,Nb)), (1,2,4,3))  # UHat = permutedims(reshape(UHat,(1152,10,16,80)), (1,2,4,3))\n",
    "    \n",
    "    B = convert(Atype, zeros(Nprimes,Nclass,Nb))  # B = convert(Atype, zeros(1152,10,80))     \n",
    "    \n",
    "    C = softMax(B, axis=2) # C is normalized along 2nd dim (classes)\n",
    "    S = C.*UHat\n",
    "    s = sum(S,1)\n",
    "    v = squash(s)\n",
    "    \n",
    "    maxiter = 1\n",
    "    for r=1:maxiter\n",
    "        A = v.* UHat\n",
    "        Agreement = sum(A,4)\n",
    "        Agreement = reshape(Agreement, (Nprimes,Nclass,Nb))  # Agreement = reshape(Agreement, (1152,10,80))\n",
    "        B = B .+ Agreement\n",
    "        \n",
    "        C = softMax(B, axis=2)\n",
    "        S = C.*UHat\n",
    "        s = sum(S,1)\n",
    "        v = squash(s)\n",
    "    end    \n",
    "    \n",
    "    yprob = safeNorm(v)\n",
    "    yprob = reshape(yprob, (Nclass,Nb))  # yprob = reshape(yprob, (10,80))\n",
    "    v = reshape(v, (Nclass,Nb,Dsecond))  # v = reshape(v, (10,80,16))\n",
    "\n",
    "    return yprob, v \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function parameters\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda = 0.5\n",
    "rloss = 0.0005;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lossCaps(w, x, y, predict; training = true)\n",
    "    yp, v = predict(w, x)\n",
    "    nb = size(x,4)\n",
    "    y1hot = conv1hot(y)\n",
    "    lossmargin = sum((abs2.(relu.(m_plus.-yp)).*y1hot)+lambda.*(abs2.(relu.(yp.-m_minus))).*(1-y1hot))\n",
    "    xmat = mat(x)\n",
    "    if training\n",
    "        vmasked = mask(v, y1hot)\n",
    "    else\n",
    "        yp1hot = conv1hot(maxidx(yp))\n",
    "        vmasked = mask(v, yp1hot)\n",
    "    end\n",
    "    xr = decode(w[6:11], vmasked)\n",
    "    lossreconstruction = sum(abs2.(xmat.-xr)) \n",
    "    return lossmargin + rloss*lossreconstruction\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function accurate(ygold, yhat)\n",
    "    correct = 0.0\n",
    "    Nb = length(ygold)\n",
    "    for i=1:Nb\n",
    "        correct += (ygold[i]==yhat[i]) ? 1.0 : 0.0\n",
    "    end\n",
    "    return correct / Nb\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function accurate(w,x,y,predict)\n",
    "    yprb, v = predict(w,x)\n",
    "    yhat = maxidx(yprb)\n",
    "    return accurate(y, yhat)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions for Capsnet\n",
    "pererror(w,data,predict) = 1 - accurate(w,data,predict)\n",
    "accurate(w,data,predict) = mean(accurate(w,x,y,predict) for (x,y) in data)\n",
    "lossCaps(w,data,predict; o...) = mean(lossCaps(w,x,y,predict;o...) for (x,y) in data)\n",
    "lossCapsgrad = grad(lossCaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainCaps(w,data,predict; epochs=3,lr=0.15,o...)\n",
    "    weights = Any[deepcopy(w)]\n",
    "    #opts = map(x->Sgd(lr=lr), w)\n",
    "    opts = map(x->Adam(), w)  # Adam optimizer\n",
    "    for epoch in 1:epochs\n",
    "        for (x,y) in data\n",
    "            g = lossCapsgrad(w,x,y,predict;o...)\n",
    "            update!(w,g,opts)\n",
    "        end\n",
    "        push!(weights,deepcopy(w))\n",
    "    end\n",
    "    return weights\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtscap = wtsinit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprb1, v1 = capsnet(wtscap,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossCaps(wtscap,x,y,capsnet;training=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurate(wtscap,x,y,capsnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = wtscap = weights = trnloss = tstloss = trnerr = tsterr = nothing; knetgc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(1)\n",
    "wtscap = wtsinit()\n",
    "@time weights = trainCaps(wtscap,dtrn,capsnet; epochs=5,lr=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time trnerr = [ pererror(w,dtrn,capsnet) for w in weights[2:end] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time tsterr = [ pererror(w,dtst,capsnet) for w in weights[2:end] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time trnloss = [ lossCaps(w,dtrn,capsnet) for w in weights[2:end] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time tstloss = [ lossCaps(w,dtst,capsnet) for w in weights[2:end] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.plot([trnerr tsterr],ylim=(0,0.1),linewidth=3,labels=[:Train_Error :Test_Error], xlabel=\"Epochs\", ylabel=\"MNIST Errors, Loss w/Reconstruction, r=1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
